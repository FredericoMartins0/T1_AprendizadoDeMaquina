{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 1: Implementção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN:\n",
    "• Algoritmo para busca do vizinho mais próximo: Força Bruta e KD Tree;\n",
    "• Métrica: Distância Euclidiana\n",
    "• k: 1, 3 ou 5 para força bruta, k=1 para KD Tree\n",
    "• Tipos de Features: Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def minkowski(r=2):\n",
    "    return lambda x,y: sum(abs(x - y)**r) ** (1/r)\n",
    "\n",
    "def knn(observation, X_treino, Y_treino, k=3, dist=minkowski(2)):\n",
    "    # dists = lista de pares (distancia, rotulo)\n",
    "    dists = [ (dist(observation, x), y) for (x,y) in list(zip(X_treino, Y_treino)) ]\n",
    "    dists.sort(key=lambda y: y[0])\n",
    "    nns = dists[:k]\n",
    "    # Extrai somente rotulos\n",
    "    labels = [ n[1] for n in nns ]\n",
    "    # Retorna lista (de tamanho 1, segundo o parametro) de elemento mais comuns, pares (rotulo, num de occorencias)\n",
    "    classification = collections.Counter(labels).most_common(1)\n",
    "    # Retorna apenas o rotulo desse unico elemento\n",
    "    return classification[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDTree:\n",
    "    #def __init__(self):\n",
    "    #    self.value = 0\n",
    "    #    self.left = None\n",
    "    #    self.right = None\n",
    "\n",
    "    def __init__(self, X_train, dim=0):\n",
    "        self.data = X_train.copy()\n",
    "\n",
    "        # Divide no meio a dimensao correspondente\n",
    "        self.median_ind = len(self.data)//2\n",
    "        self.data.sort(axis=dim) \n",
    "        self.value = self.data[self.median_ind]\n",
    "\n",
    "        self.max_dim = len(self.data[0])\n",
    "        self.next_dim = (dim+1) % self.max_dim\n",
    "\n",
    "        # Cria nodos filhos caso possivel\n",
    "        self.left, self.right = None, None\n",
    "        if self.median_ind > 0:\n",
    "            self.left_split  = self.data[:self.median_ind]\n",
    "            self.left  = KDTree(self.left_split,  dim = self.next_dim)\n",
    "\n",
    "        if len(self.data)-self.median_ind-1 > 0:\n",
    "            self.right_split = self.data[self.median_ind+1:]\n",
    "            self.right  = KDTree(self.right_split,  dim = self.next_dim)\n",
    "\n",
    "        print(self.left, self.value, self.right)\n",
    "    \n",
    "\n",
    "\n",
    "def knn_kdtree():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = normalized_train[:4]\n",
    "print(temp)\n",
    "kd = KDTree(temp)\n",
    "#kd = KDTree()\n",
    "#kd.create_tree(temp)\n",
    "#print(normalized_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "def prior(y,dataset): # P(y = c)\n",
    "    total = len(dataset)\n",
    "    vals = dataset[y].unique()\n",
    "    arr = []\n",
    "    for i in range(len(vals)):\n",
    "        prob = dataset[y].value_counts(vals[i])/total\n",
    "        arr.append((vals[i],prob))\n",
    "    return arr\n",
    "\n",
    "def likelihood(y,dataset): # P(x | y = c)\n",
    "    like = 1\n",
    "    for i in np.arange(0,dataset.shape[1]):\n",
    "        mu = np.mean(dataset[:,i])\n",
    "        sigma = np.std(dataset[:,i])\n",
    "        gauss = random.normal(loc=mu,scale=sigma,size=y.shape)\n",
    "        #gauss = (1/np.sqrt(2*np.pi*(sigma**2)))*np.exp(-1*((y[i]-mu)**2)/(2*(sigma**2)))\n",
    "        like = like * gauss\n",
    "    return like\n",
    "    \n",
    "\"\"\"\n",
    "def likelyhood(y, Z):\n",
    "    def gaussian(x, mu, sig):\n",
    "        return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "    prob = 1\n",
    "    for j in np.arange(0, Z.shape[1]):\n",
    "        m = np.mean(Z[:,j])\n",
    "        s = np.std(Z[:,j])      \n",
    "        prob = prob*gaussian(y[j], m, s)\n",
    "    return prob\n",
    "\"\"\"\n",
    "\n",
    "def evidence(y,dataset): # P(x)\n",
    "    arr = prior(y,dataset)\n",
    "    like = likelihood(y,dataset)\n",
    "    ev = 0\n",
    "    for tp in arr:\n",
    "        ev = ev + (like*tp[1])\n",
    "    return ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_features(X_train):\n",
    "    return [col for col in X_train.columns if X_train[col].dtypes != 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(dataset,y):\n",
    "    like = 1\n",
    "    ev = 1\n",
    "    for i in get_numerical_features(y):\n",
    "        like = like * likelihood(y,dataset)\n",
    "        ev = evidence(i,dataset)\n",
    "    return (prior(y,dataset)*like(y,dataset))/ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 2: Treinamento e Inferência\n",
    "\n",
    "Primeiramente deve-se organizar os dados e dividí-los entre dados de treino e dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame(pd.read_csv('../penguins.csv'))\n",
    "\n",
    "# Retirar valores invalidos\n",
    "data.replace(\"Na\",float(\"NaN\"),inplace=True)\n",
    "data.dropna(subset=[\"Culmen Length (mm)\",\"Culmen Depth (mm)\",\"Flipper Length (mm)\",\"Body Mass (g)\",\"Sex\",\"Delta 15 N (o/oo)\",\"Delta 13 C (o/oo)\"],inplace=True)\n",
    "\n",
    "# Shuffle dos dados não balanceados\n",
    "#data = data.sample(frac=1)\n",
    "\n",
    "# Separar atributo alvo dos restantes\n",
    "y = data[\"Species\"]\n",
    "x = data.drop(columns=\"Species\")\n",
    "\n",
    "# Selecionar apenas colunas numericas relevantes\n",
    "x = x.drop(columns=[\"studyName\", \"Sample Number\", \"Region\", \"Island\", \"Stage\", \"Individual ID\", \"Clutch Completion\", \"Date Egg\", \"Sex\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\", \"Comments\"])\n",
    "\n",
    "# Split dos dados\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, shuffle=True)\n",
    "\n",
    "#split = 0.7\n",
    "#ind_split = int(len(data) * split)\n",
    "#\n",
    "#x_treino = x[:ind_split]\n",
    "#y_treino = y[:ind_split]\n",
    "#\n",
    "#x_teste = x[ind_split:]\n",
    "#y_teste = y[ind_split:]\n",
    "\n",
    "# Normalizar dados\n",
    "scaler = StandardScaler()\n",
    "normalized_train = scaler.fit_transform(x_train)\n",
    "\n",
    "normalized_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(normalized_train, y_train.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kd a tree???\n",
    "\n",
    "# achei a tree\n",
    "#aux = knn(normalized_test.iloc[[0]],x_teste,y_teste)\n",
    "#print(np.append(x_teste, y_teste[..., np.newaxis], axis=1))\n",
    "\n",
    "#umteste = normalized_test[0]\n",
    "#y_hat = knn(umteste, normalized_train, y_train.to_numpy())\n",
    "for (x,y) in zip(normalized_test, y_test.to_numpy()):\n",
    "    y_hat = knn(x, normalized_train, y_train.to_numpy(), k=3)\n",
    "    #print(y_hat, x)\n",
    "    print(y_hat == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3621\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-9baa4e0999c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mx_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnaive_bayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#P = pd.DataFrame(data=np.zeros((x_test.shape[0], len(classes))), columns = classes)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#for i in np.arange(0, len(classes)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-d971052f1b10>\u001b[0m in \u001b[0;36mnaive_bayes\u001b[1;34m(dataset, y)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mget_numerical_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mlike\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlike\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevidence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-85-4f59772f8f03>\u001b[0m in \u001b[0;36mlikelihood\u001b[1;34m(y, Z)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgaussian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3623\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3624\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3625\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "for (x,y) in zip(normalized_test, x_test.to_numpy()):\n",
    "    x_hat = naive_bayes(normalized_train,x_train)\n",
    "    print(x_hat)\n",
    "#P = pd.DataFrame(data=np.zeros((x_test.shape[0], len(classes))), columns = classes) \n",
    "#for i in np.arange(0, len(classes)):\n",
    "#    elements = tuple(np.where(y_train == classes[i]))\n",
    "#    Z = x_train[elements,:][0]\n",
    "#    for j in np.arange(0,x_test.shape[0]):\n",
    "#        x = x_test[j,:]\n",
    "#        pj = likelihood(x,Z)\n",
    "#        P[classes[i]][j] = pj*len(elements)/x_train.shape[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2302e95b8ae7d8a7bd3b88c4be156fe5ce951f0b350a984295d2ca7196f8fc64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
