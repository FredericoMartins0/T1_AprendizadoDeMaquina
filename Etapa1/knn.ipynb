{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 1: Implementção"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN:\n",
    "• Algoritmo para busca do vizinho mais próximo: Força Bruta e KD Tree;\n",
    "• Métrica: Distância Euclidiana\n",
    "• k: 1, 3 ou 5 para força bruta, k=1 para KD Tree\n",
    "• Tipos de Features: Numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def minkowski(r=2):\n",
    "    return lambda x,y: sum(abs(x - y)**r) ** (1/r)\n",
    "\n",
    "def knn(observation, X_treino, Y_treino, k=3, dist=minkowski(2)):\n",
    "    # dists = lista de pares (distancia, rotulo)\n",
    "    dists = [ (dist(observation, x), y) for (x,y) in list(zip(X_treino, Y_treino)) ]\n",
    "    dists.sort(key=lambda y: y[0])\n",
    "    nns = dists[:k]\n",
    "    # Extrai somente rotulos\n",
    "    labels = [ n[1] for n in nns ]\n",
    "    # Retorna lista (de tamanho 1, segundo o parametro) de elemento mais comuns, pares (rotulo, num de occorencias)\n",
    "    classification = collections.Counter(labels).most_common(1)\n",
    "    # Retorna apenas o rotulo desse unico elemento\n",
    "    return classification[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDTree:\n",
    "    #def __init__(self):\n",
    "    #    self.value = 0\n",
    "    #    self.left = None\n",
    "    #    self.right = None\n",
    "\n",
    "    def __init__(self, X_train, dim=0):\n",
    "        self.data = X_train.copy()\n",
    "\n",
    "        # Divide no meio a dimensao correspondente\n",
    "        self.median_ind = len(self.data)//2\n",
    "        self.data.sort(axis=dim) \n",
    "        self.value = self.data[self.median_ind]\n",
    "\n",
    "        self.max_dim = len(self.data[0])\n",
    "        self.next_dim = (dim+1) % self.max_dim\n",
    "\n",
    "        # Cria nodos filhos caso possivel\n",
    "        self.left, self.right = None, None\n",
    "        if self.median_ind > 0:\n",
    "            self.left_split  = self.data[:self.median_ind]\n",
    "            self.left  = KDTree(self.left_split,  dim = self.next_dim)\n",
    "\n",
    "        if len(self.data)-self.median_ind-1 > 0:\n",
    "            self.right_split = self.data[self.median_ind+1:]\n",
    "            self.right  = KDTree(self.right_split,  dim = self.next_dim)\n",
    "\n",
    "        print(self.left, self.value, self.right)\n",
    "    \n",
    "\n",
    "\n",
    "def knn_kdtree():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6ad50892188c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalized_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#kd = KDTree()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#kd.create_tree(temp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normalized_train' is not defined"
     ]
    }
   ],
   "source": [
    "temp = normalized_train[:4]\n",
    "print(temp)\n",
    "kd = KDTree(temp)\n",
    "#kd = KDTree()\n",
    "#kd.create_tree(temp)\n",
    "#print(normalized_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(y,dataset): # P(y = c)\n",
    "    total = len(dataset)\n",
    "    vals = dataset[y].unique()\n",
    "    arr = []\n",
    "    for i in range(len(vals)):\n",
    "        prob = dataset[y].value_counts(vals[i])/total\n",
    "        arr.append((vals[i],prob))\n",
    "    return arr\n",
    "\n",
    "def likelihood(y,dataset): # P(x | y = c)\n",
    "    like = 1\n",
    "    for i in np.arange(0,dataset.shape(1)):\n",
    "        mu = np.mean(dataset[:i])\n",
    "        sigma = np.std(dataset[:i])\n",
    "        gauss = (1/np.sqrt(2*np.pi*(sigma**2)))*np.exp(-1*((y[i]-mu)**2)/(2*(sigma**2)))\n",
    "        like = like * gauss\n",
    "    return like\n",
    "\n",
    "def evidence(y,dataset): # P(x)\n",
    "    arr = prior(y,dataset)\n",
    "    like = likelihood(y,dataset)\n",
    "    ev = 0\n",
    "    for tp in arr:\n",
    "        ev = ev + (like*tp[1])\n",
    "    return ev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_features(x_train):\n",
    "    return [col for col in x_train.columns if x_train[col].dtypes != 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(dataset,y):\n",
    "    like = 1\n",
    "    ev = 1\n",
    "    for i in get_numerical_features(y):\n",
    "        like = like * likelihood(i,dataset)\n",
    "        ev = evidence(i,dataset)\n",
    "    return (prior(y,dataset)*like(y,dataset))/ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etapa 2: Treinamento e Inferência\n",
    "\n",
    "Primeiramente deve-se organizar os dados e dividí-los entre dados de treino e dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Culmen Length (mm)',\n",
       " 'Culmen Depth (mm)',\n",
       " 'Flipper Length (mm)',\n",
       " 'Body Mass (g)']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.DataFrame(pd.read_csv('../penguins.csv'))\n",
    "\n",
    "# Retirar valores invalidos\n",
    "data.replace(\"Na\",float(\"NaN\"),inplace=True)\n",
    "data.dropna(subset=[\"Culmen Length (mm)\",\"Culmen Depth (mm)\",\"Flipper Length (mm)\",\"Body Mass (g)\",\"Sex\",\"Delta 15 N (o/oo)\",\"Delta 13 C (o/oo)\"],inplace=True)\n",
    "\n",
    "# Shuffle dos dados não balanceados\n",
    "#data = data.sample(frac=1)\n",
    "\n",
    "# Separar atributo alvo dos restantes\n",
    "y = data[\"Species\"]\n",
    "x = data.drop(columns=\"Species\")\n",
    "\n",
    "# Selecionar apenas colunas numericas relevantes\n",
    "x = x.drop(columns=[\"studyName\", \"Sample Number\", \"Region\", \"Island\", \"Stage\", \"Individual ID\", \"Clutch Completion\", \"Date Egg\", \"Sex\", \"Delta 15 N (o/oo)\", \"Delta 13 C (o/oo)\", \"Comments\"])\n",
    "\n",
    "# Split dos dados\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, shuffle=True)\n",
    "\n",
    "#split = 0.7\n",
    "#ind_split = int(len(data) * split)\n",
    "#\n",
    "#x_treino = x[:ind_split]\n",
    "#y_treino = y[:ind_split]\n",
    "#\n",
    "#x_teste = x[ind_split:]\n",
    "#y_teste = y[ind_split:]\n",
    "\n",
    "# Normalizar dados\n",
    "scaler = StandardScaler()\n",
    "normalized_train = scaler.fit_transform(x_train)\n",
    "\n",
    "normalized_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(normalized_train, y_train.to_numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# kd a tree???\n",
    "\n",
    "# achei a tree\n",
    "#aux = knn(normalized_test.iloc[[0]],x_teste,y_teste)\n",
    "#print(np.append(x_teste, y_teste[..., np.newaxis], axis=1))\n",
    "\n",
    "#umteste = normalized_test[0]\n",
    "#y_hat = knn(umteste, normalized_train, y_train.to_numpy())\n",
    "for (x,y) in zip(normalized_test, y_test.to_numpy()):\n",
    "    y_hat = knn(x, normalized_train, y_train.to_numpy(), k=3)\n",
    "    #print(y_hat, x)\n",
    "    print(y_hat == y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2302e95b8ae7d8a7bd3b88c4be156fe5ce951f0b350a984295d2ca7196f8fc64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
